{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infant Respiration Estimation - Vollständige Pipeline\n",
    "\n",
    "Dieses Notebook enthält alle erforderlichen Schritte zum Ausführen des Infant Respiration Estimation Projekts.\n",
    "\n",
    "## Überblick\n",
    "\n",
    "Dieses Projekt implementiert automatische Atmungsschätzung bei Säuglingen aus Videos unter Verwendung von Deep Learning-Methoden, insbesondere dem **AIRFlowNet**-Modell.\n",
    "\n",
    "**Paper:** [Automatic Infant Respiration Estimation from Video: A Deep Flow-based Algorithm and a Novel Public Benchmark](https://arxiv.org/pdf/2307.13110.pdf)\n",
    "\n",
    "### Unterstützte Modelle:\n",
    "- **VIRENet** (AIRFlowNet)\n",
    "- DeepPhys\n",
    "- EfficientPhys\n",
    "- TS-CAN\n",
    "\n",
    "### Datasets:\n",
    "- **AIR-125**: 125 annotierte Säuglingsvideos\n",
    "- **COHFACE**: Erwachsenen-Atmungsdatensatz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Umgebungsüberprüfung und Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Prüfe Python-Version\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "print(f\"Current Working Directory: {os.getcwd()}\")\n",
    "\n",
    "# Prüfe CUDA-Verfügbarkeit\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"\\nPyTorch Version: {torch.__version__}\")\n",
    "    print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "        print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "        print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "except ImportError:\n",
    "    print(\"\\nPyTorch ist noch nicht installiert.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Installation der Abhängigkeiten\n",
    "\n",
    "**Hinweis:** Dieser Schritt kann einige Minuten dauern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation der erforderlichen Pakete\n",
    "!pip install -q h5py==2.10.0\n",
    "!pip install -q yacs==0.1.8\n",
    "!pip install -q scipy==1.5.2\n",
    "!pip install -q pandas==1.1.5\n",
    "!pip install -q scikit-image==0.17.2\n",
    "!pip install -q numpy==1.22.0\n",
    "!pip install -q matplotlib==3.1.2\n",
    "!pip install -q opencv-python==4.5.2.54\n",
    "!pip install -q PyYAML==6.0\n",
    "!pip install -q scikit-learn==1.0.2\n",
    "!pip install -q tensorboardX==2.4.1\n",
    "!pip install -q tqdm==4.64.0\n",
    "!pip install -q mat73==0.59\n",
    "\n",
    "# Für PyTorch (falls noch nicht installiert)\n",
    "# !pip install torch==1.12.1 torchvision==0.13.1 torchaudio==0.12.1\n",
    "\n",
    "print(\"\\nInstallation abgeschlossen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Download und Extraktion\n",
    "\n",
    "Der AIR-125 Dataset wird von der Northeastern University heruntergeladen und extrahiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Definiere Pfade\n",
    "dataset_url = \"https://coe.northeastern.edu/Research/AClab/AIR-125/AIR.zip\"\n",
    "download_path = Path(\"./data/AIR.zip\")\n",
    "extract_path = Path(\"./data/AIR\")\n",
    "\n",
    "# Erstelle Verzeichnis\n",
    "download_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Download mit Fortschrittsanzeige\n",
    "class DownloadProgressBar(tqdm):\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)\n",
    "\n",
    "if not download_path.exists():\n",
    "    print(f\"Downloading AIR-125 dataset from {dataset_url}...\")\n",
    "    print(\"Dies kann mehrere Minuten dauern, je nach Internetgeschwindigkeit.\")\n",
    "    \n",
    "    with DownloadProgressBar(unit='B', unit_scale=True, miniters=1, desc='AIR.zip') as t:\n",
    "        urllib.request.urlretrieve(dataset_url, download_path, reporthook=t.update_to)\n",
    "    print(f\"\\nDownload abgeschlossen: {download_path}\")\n",
    "else:\n",
    "    print(f\"Dataset bereits heruntergeladen: {download_path}\")\n",
    "\n",
    "# Extrahiere ZIP-Datei\n",
    "if not extract_path.exists():\n",
    "    print(f\"\\nExtrahiere Dataset nach {extract_path}...\")\n",
    "    with zipfile.ZipFile(download_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path.parent)\n",
    "    print(\"Extraktion abgeschlossen!\")\n",
    "else:\n",
    "    print(f\"Dataset bereits extrahiert: {extract_path}\")\n",
    "\n",
    "# Zeige Dataset-Struktur\n",
    "print(\"\\nDataset-Struktur:\")\n",
    "for item in sorted(extract_path.rglob('*'))[:10]:\n",
    "    print(f\"  {item.relative_to(extract_path.parent)}\")\n",
    "print(\"  ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset-Inspektion\n",
    "\n",
    "Lass uns einen Blick auf die Dataset-Struktur und einige Sample-Daten werfen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Zähle Dateien im Dataset\n",
    "dataset_path = Path(\"./data/AIR\")\n",
    "\n",
    "if dataset_path.exists():\n",
    "    video_files = list(dataset_path.rglob('*.avi')) + list(dataset_path.rglob('*.mp4'))\n",
    "    mat_files = list(dataset_path.rglob('*.mat'))\n",
    "    \n",
    "    print(f\"Anzahl Video-Dateien: {len(video_files)}\")\n",
    "    print(f\"Anzahl MAT-Dateien (Annotationen): {len(mat_files)}\")\n",
    "    \n",
    "    if video_files:\n",
    "        print(\"\\nBeispiel Video-Dateien:\")\n",
    "        for vf in video_files[:5]:\n",
    "            print(f\"  - {vf.name}\")\n",
    "    \n",
    "    # Versuche ein Video zu laden und ein Frame anzuzeigen\n",
    "    if video_files:\n",
    "        sample_video = video_files[0]\n",
    "        print(f\"\\nLade Sample-Video: {sample_video.name}\")\n",
    "        \n",
    "        cap = cv2.VideoCapture(str(sample_video))\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if ret:\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.imshow(frame_rgb)\n",
    "            plt.title(f\"Sample Frame from {sample_video.name}\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            \n",
    "            # Video-Informationen\n",
    "            fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "            \n",
    "            print(f\"\\nVideo-Informationen:\")\n",
    "            print(f\"  - FPS: {fps}\")\n",
    "            print(f\"  - Frames: {frame_count}\")\n",
    "            print(f\"  - Auflösung: {width}x{height}\")\n",
    "            print(f\"  - Dauer: {frame_count/fps:.2f} Sekunden\")\n",
    "        \n",
    "        cap.release()\n",
    "else:\n",
    "    print(f\"Dataset-Pfad nicht gefunden: {dataset_path}\")\n",
    "    print(\"Bitte führe Zelle 3 aus, um das Dataset herunterzuladen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Konfiguration erstellen\n",
    "\n",
    "Wir erstellen eine Konfigurationsdatei für das Training mit dem AIR-125 Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "# Definiere Pfade\n",
    "raw_data_path = str(Path(\"./data/AIR\").absolute())\n",
    "processed_data_path = str(Path(\"./data/processed_AIR\").absolute())\n",
    "\n",
    "# Erstelle processed data Verzeichnis\n",
    "Path(processed_data_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Erstelle Konfiguration\n",
    "config = {\n",
    "    'BASE': [''],\n",
    "    'TOOLBOX_MODE': 'train_and_test',\n",
    "    'TRAIN': {\n",
    "        'BATCH_SIZE': 4,\n",
    "        'EPOCHS': 10,\n",
    "        'LR': 0.001,\n",
    "        'MODEL_FILE_NAME': 'AIR_VIRENet_model',\n",
    "        'DATA': {\n",
    "            'FS': 5,\n",
    "            'DATASET': 'AIR',\n",
    "            'DO_PREPROCESS': True,\n",
    "            'DATA_FORMAT': 'NDCHW',\n",
    "            'DATA_PATH': raw_data_path,\n",
    "            'CACHED_PATH': processed_data_path,\n",
    "            'EXP_DATA_NAME': '',\n",
    "            'BEGIN': 0.0,\n",
    "            'END': 0.7,\n",
    "            'PREPROCESS': {\n",
    "                'DATA_TYPE': ['Standardized'],\n",
    "                'LABEL_TYPE': 'Standardized',\n",
    "                'DO_CHUNK': True,\n",
    "                'CHUNK_LENGTH': 60,\n",
    "                'DYNAMIC_DETECTION': False,\n",
    "                'DYNAMIC_DETECTION_FREQUENCY': 60,\n",
    "                'CROP_FACE': False,\n",
    "                'LARGE_FACE_BOX': True,\n",
    "                'LARGE_BOX_COEF': 1.5,\n",
    "                'H': 96,\n",
    "                'W': 96\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'VALID': {\n",
    "        'DATA': {\n",
    "            'FS': 5,\n",
    "            'DATASET': 'AIR',\n",
    "            'DO_PREPROCESS': True,\n",
    "            'DATA_FORMAT': 'NDCHW',\n",
    "            'DATA_PATH': raw_data_path,\n",
    "            'CACHED_PATH': processed_data_path,\n",
    "            'EXP_DATA_NAME': '',\n",
    "            'BEGIN': 0.7,\n",
    "            'END': 1.0,\n",
    "            'PREPROCESS': {\n",
    "                'DATA_TYPE': ['Standardized'],\n",
    "                'LABEL_TYPE': 'Standardized',\n",
    "                'DO_CHUNK': True,\n",
    "                'CHUNK_LENGTH': 60,\n",
    "                'DYNAMIC_DETECTION': False,\n",
    "                'DYNAMIC_DETECTION_FREQUENCY': 60,\n",
    "                'CROP_FACE': False,\n",
    "                'LARGE_FACE_BOX': True,\n",
    "                'LARGE_BOX_COEF': 1.5,\n",
    "                'H': 96,\n",
    "                'W': 96\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'TEST': {\n",
    "        'METRICS': ['MAE', 'RMSE', 'MAPE', 'Pearson'],\n",
    "        'USE_LAST_EPOCH': False,\n",
    "        'DATA': {\n",
    "            'FS': 5,\n",
    "            'DATASET': 'AIR',\n",
    "            'DO_PREPROCESS': True,\n",
    "            'DATA_FORMAT': 'NDCHW',\n",
    "            'DATA_PATH': raw_data_path,\n",
    "            'CACHED_PATH': processed_data_path,\n",
    "            'EXP_DATA_NAME': '',\n",
    "            'BEGIN': 0.0,\n",
    "            'END': 1.0,\n",
    "            'PREPROCESS': {\n",
    "                'DATA_TYPE': ['Standardized'],\n",
    "                'LABEL_TYPE': 'Standardized',\n",
    "                'DO_CHUNK': True,\n",
    "                'CHUNK_LENGTH': 60,\n",
    "                'DYNAMIC_DETECTION': False,\n",
    "                'DYNAMIC_DETECTION_FREQUENCY': 60,\n",
    "                'CROP_FACE': False,\n",
    "                'LARGE_FACE_BOX': True,\n",
    "                'LARGE_BOX_COEF': 1.5,\n",
    "                'H': 96,\n",
    "                'W': 96\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'DEVICE': 'cuda:0',\n",
    "    'NUM_OF_GPU_TRAIN': 1,\n",
    "    'LOG': {\n",
    "        'PATH': 'runs/exp'\n",
    "    },\n",
    "    'MODEL': {\n",
    "        'DROP_RATE': 0.2,\n",
    "        'NAME': 'VIRENet',\n",
    "        'MODEL_DIR': '',\n",
    "        'VIRENET': {\n",
    "            'FRAME_DEPTH': 10\n",
    "        }\n",
    "    },\n",
    "    'INFERENCE': {\n",
    "        'BATCH_SIZE': 4,\n",
    "        'EVALUATION_METHOD': 'FFT',\n",
    "        'MODEL_PATH': ''\n",
    "    }\n",
    "}\n",
    "\n",
    "# Speichere Konfiguration\n",
    "config_path = Path('./notebook_config.yaml')\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(config, f, default_flow_style=False)\n",
    "\n",
    "print(f\"Konfiguration erstellt: {config_path.absolute()}\")\n",
    "print(f\"\\nDataset-Pfad: {raw_data_path}\")\n",
    "print(f\"Verarbeiteter Dataset-Pfad: {processed_data_path}\")\n",
    "print(f\"\\nKonfiguration:\")\n",
    "print(f\"  - Modell: {config['MODEL']['NAME']}\")\n",
    "print(f\"  - Batch Size: {config['TRAIN']['BATCH_SIZE']}\")\n",
    "print(f\"  - Epochen: {config['TRAIN']['EPOCHS']}\")\n",
    "print(f\"  - Learning Rate: {config['TRAIN']['LR']}\")\n",
    "print(f\"  - Device: {config['DEVICE']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Training\n",
    "\n",
    "Jetzt trainieren wir das VIRENet (AIRFlowNet) Modell auf dem AIR-125 Dataset.\n",
    "\n",
    "**Hinweis:** \n",
    "- Das Training kann je nach Hardware mehrere Stunden dauern\n",
    "- GPU wird empfohlen (CUDA)\n",
    "- Der erste Run führt Preprocessing durch (DO_PREPROCESS: True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Überprüfe ob das Hauptverzeichnis korrekt ist\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Falls wir nicht im Hauptverzeichnis sind, wechsle dorthin\n",
    "repo_root = Path.cwd()\n",
    "if not (repo_root / 'main.py').exists():\n",
    "    # Suche nach main.py im Parent-Verzeichnis\n",
    "    if (repo_root.parent / 'main.py').exists():\n",
    "        repo_root = repo_root.parent\n",
    "        os.chdir(repo_root)\n",
    "\n",
    "print(f\"Working Directory: {os.getcwd()}\")\n",
    "print(f\"Config File: {Path('./notebook_config.yaml').absolute()}\")\n",
    "\n",
    "# Füge das Repository zum Python-Pfad hinzu\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "print(\"\\nStarte Training...\")\n",
    "print(\"Dies kann mehrere Stunden dauern.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Führe Training aus\n",
    "!python main.py --config_file ./notebook_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Testing / Inference\n",
    "\n",
    "Nach dem Training können wir das Modell auf Testdaten evaluieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstelle Test-Konfiguration\n",
    "# Kopiere die bestehende Konfiguration und ändere den Modus auf \"only_test\"\n",
    "\n",
    "with open('./notebook_config.yaml', 'r') as f:\n",
    "    test_config = yaml.safe_load(f)\n",
    "\n",
    "test_config['TOOLBOX_MODE'] = 'only_test'\n",
    "\n",
    "# Setze den Pfad zum trainierten Modell\n",
    "# Dies muss nach dem Training aktualisiert werden\n",
    "model_path = Path('./runs/exp/AIR_VIRENet_model')  # Passe den Pfad an\n",
    "\n",
    "if model_path.exists():\n",
    "    # Finde das beste oder letzte Modell\n",
    "    model_files = list(model_path.glob('*.pth'))\n",
    "    if model_files:\n",
    "        latest_model = max(model_files, key=os.path.getctime)\n",
    "        test_config['INFERENCE']['MODEL_PATH'] = str(latest_model)\n",
    "        print(f\"Verwende Modell: {latest_model}\")\n",
    "    else:\n",
    "        print(\"Keine .pth Modell-Dateien gefunden.\")\n",
    "else:\n",
    "    print(f\"Modell-Verzeichnis nicht gefunden: {model_path}\")\n",
    "    print(\"Bitte führe zuerst das Training aus.\")\n",
    "\n",
    "# Speichere Test-Konfiguration\n",
    "test_config_path = Path('./notebook_test_config.yaml')\n",
    "with open(test_config_path, 'w') as f:\n",
    "    yaml.dump(test_config, f, default_flow_style=False)\n",
    "\n",
    "print(f\"\\nTest-Konfiguration erstellt: {test_config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Führe Testing aus\n",
    "!python main.py --config_file ./notebook_test_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ergebnisse visualisieren\n",
    "\n",
    "Lass uns die Trainingsergebnisse und Metriken visualisieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Finde die Ergebnisdateien\n",
    "results_dir = Path('./runs/exp')\n",
    "\n",
    "if results_dir.exists():\n",
    "    # Suche nach CSV-Dateien mit Metriken\n",
    "    csv_files = list(results_dir.rglob('*.csv'))\n",
    "    \n",
    "    if csv_files:\n",
    "        print(f\"Gefundene Ergebnisdateien: {len(csv_files)}\")\n",
    "        \n",
    "        for csv_file in csv_files[:3]:  # Zeige die ersten 3\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Datei: {csv_file.name}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            try:\n",
    "                df = pd.read_csv(csv_file)\n",
    "                print(f\"\\n{df.to_string()}\")\n",
    "                \n",
    "                # Wenn die Datei numerische Daten enthält, plotte sie\n",
    "                numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "                if len(numeric_cols) > 0:\n",
    "                    fig, axes = plt.subplots(1, min(len(numeric_cols), 3), figsize=(15, 4))\n",
    "                    if len(numeric_cols) == 1:\n",
    "                        axes = [axes]\n",
    "                    \n",
    "                    for idx, col in enumerate(numeric_cols[:3]):\n",
    "                        axes[idx].plot(df[col])\n",
    "                        axes[idx].set_title(col)\n",
    "                        axes[idx].set_xlabel('Index')\n",
    "                        axes[idx].set_ylabel('Value')\n",
    "                        axes[idx].grid(True)\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "            except Exception as e:\n",
    "                print(f\"Fehler beim Laden von {csv_file.name}: {e}\")\n",
    "    else:\n",
    "        print(\"Keine CSV-Ergebnisdateien gefunden.\")\n",
    "else:\n",
    "    print(f\"Ergebnisverzeichnis nicht gefunden: {results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. TensorBoard (optional)\n",
    "\n",
    "Falls TensorBoard-Logs erstellt wurden, können wir diese visualisieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lade TensorBoard in Jupyter\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Starte TensorBoard\n",
    "log_dir = './runs/exp'\n",
    "print(f\"Starte TensorBoard mit Log-Verzeichnis: {log_dir}\")\n",
    "%tensorboard --logdir {log_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Verschiedene Modelle ausprobieren\n",
    "\n",
    "Du kannst auch andere Modelle ausprobieren, indem du die Konfiguration änderst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verfügbare Modelle\n",
    "available_models = {\n",
    "    'VIRENet': 'AIRFlowNet - Flow-basiertes Modell (empfohlen für Säuglinge)',\n",
    "    'DeepPhys': 'DeepPhys - Attention-basiertes Modell',\n",
    "    'EfficientPhys': 'EfficientPhys - Effizientes Modell',\n",
    "    'Tscan': 'TS-CAN - Temporal Shift Attention Network'\n",
    "}\n",
    "\n",
    "print(\"Verfügbare Modelle:\")\n",
    "print(\"=\"*60)\n",
    "for model_name, description in available_models.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  {description}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nUm ein anderes Modell zu verwenden:\")\n",
    "print(\"1. Ändere config['MODEL']['NAME'] in Zelle 5\")\n",
    "print(\"2. Führe die Training-Zellen erneut aus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Zusammenfassung und nächste Schritte\n",
    "\n",
    "### Was wir getan haben:\n",
    "1. Umgebung eingerichtet und Abhängigkeiten installiert\n",
    "2. AIR-125 Dataset heruntergeladen und extrahiert\n",
    "3. Konfigurationsdatei erstellt\n",
    "4. Modell trainiert\n",
    "5. Modell getestet und evaluiert\n",
    "6. Ergebnisse visualisiert\n",
    "\n",
    "### Nächste Schritte:\n",
    "- **Hyperparameter-Tuning**: Experimentiere mit verschiedenen Lernraten, Batch-Größen, etc.\n",
    "- **Andere Modelle**: Probiere DeepPhys, EfficientPhys oder TS-CAN aus\n",
    "- **COHFACE Dataset**: Evaluiere auf dem Erwachsenen-Dataset\n",
    "- **Optical Flow**: Generiere Optical Flow Inputs für verbesserte Ergebnisse\n",
    "- **Eigene Videos**: Teste das Modell auf eigenen Säuglingsvideos\n",
    "\n",
    "### Wichtige Konfigurationsparameter:\n",
    "```yaml\n",
    "TRAIN:\n",
    "  BATCH_SIZE: 4          # Batch-Größe (größer = schneller, aber mehr GPU-Memory)\n",
    "  EPOCHS: 10             # Anzahl Trainings-Epochen\n",
    "  LR: 0.001              # Lernrate\n",
    "\n",
    "PREPROCESS:\n",
    "  CHUNK_LENGTH: 60       # Länge der Video-Chunks in Frames\n",
    "  H: 96                  # Höhe der verarbeiteten Frames\n",
    "  W: 96                  # Breite der verarbeiteten Frames\n",
    "  CROP_FACE: False       # Gesichtserkennung aktivieren/deaktivieren\n",
    "\n",
    "MODEL:\n",
    "  NAME: VIRENet          # Modellname (VIRENet, DeepPhys, EfficientPhys, Tscan)\n",
    "```\n",
    "\n",
    "### Referenzen:\n",
    "- [Paper](https://arxiv.org/pdf/2307.13110.pdf)\n",
    "- [GitHub Repository](https://github.com/ubicomplab/rPPG-Toolbox)\n",
    "- [Dataset](https://coe.northeastern.edu/Research/AClab/AIR-125/)\n",
    "\n",
    "### Support:\n",
    "Bei Fragen oder Problemen, siehe README.md oder öffne ein Issue auf GitHub."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
